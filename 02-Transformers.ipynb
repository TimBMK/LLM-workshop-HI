{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomer Models\n",
    "\n",
    "This notebook uses the *transformers* library. It is a unified framework for deploying models and developed by [huggingface](https://huggingface.co/). Among other things, it provides functionality to download, train and run language models. You can find a list of models in the [model hub](https://huggingface.co/models). Note that the model hub also contains large language models often not suitable for local deployment. Instead, it has become common to run LLMs remotely via APIs. We will look into frameworks for this later.\n",
    "\n",
    "The *transformers* library is commonly used to work with models deployed locally. While this means that we can often not run Large Language Models on consumer-grade hardware, it is perfectly suitable for smaller models such as BERT, which are already quite powerful and perectly suited for a lot of tasks - especially when fine-tuned. If you run the *transformers* library on a more powerful server (e.g. AWS, Google or Azure), however, it allows you to run very large models. We will look into deploying quantized LLMs, which circumvent some of these limitations, later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# define the model name here for convenience\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "# load model and tokenizer. downloads the model if necessary\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *tokenizer* translates the words of a given input to the numbers associated with it by the model. This is necessary, as the actual model only deals in numbers - not text input. This is basically the same mechanism as the `index_to_key` method in our static embeddings before. \n",
    "\n",
    "**Important**: tokenizer and model need to match, otherwise the model does not associate the right embeddings with the words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the model. Notice how it differentiates between the static word embeddings, additional  embeddings (such as the position in the sequence), and the Encoder. The Attention layer in the Encoder here has 12 stacked  (\"BertLayers\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static word embeddings here have a vocabulary size of 30.522 over 768 embedding dimensions. While it may seem counter-intuitive that BERT has a smaller vocabulary size than the less powerful word2vec model, this can be easily explained: In BERT (and similar models), words are broken town into subwords before embedding. This makes the embedding more flexible and performant, and gives it a better chance at recognizing out-of-vocabulary words!\n",
    "\n",
    "For example, In Word2Vec, the words \"running\", \"runner\", and \"runs\" would each be separate tokens.\n",
    "In BERT, these words might be tokenized into subwords like \"run\", \"##ning\", \"##ner\", and \"##s\", where \"##\" indicates a subword that is part of a larger word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualized Embeddings\n",
    "\n",
    "Let's see how the BERT model has different embeddings for the same word (\"bank\") in different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2066,  2300,  1010,  2061,  1045,  2788,  3328,  2011,\n",
       "          1996,  2314,  2924,  1012,   102],\n",
       "        [  101,  1045,  2031,  2000,  2175,  2011,  1996,  2924,  1998, 10632,\n",
       "          2070,  2769,  1012,   102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"I like water, so I usually walk by the river bank.\",\n",
    "             \"I have to go by the bank and withdraw some money.\"\n",
    "]\n",
    "\n",
    "# we need to tokenzie the sentences before feeding them to the \n",
    "\n",
    "input = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: A \"tensor\" object is something like a multidimensional matrix. While this is not technically correct, it is mostly used this way in data science. If you want to find out what a tensor actually is, I recommend reading up on linear algebra, or ask somebody knowledgable, like a physicist or mathematician. But be aware that physicists and mathematicians might disagree on the nature of a tensor...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the words in the sequence, we need to convert the IDs back to tokens. Note how the tokenizer added special tokens in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'i',\n",
       " 'like',\n",
       " 'water',\n",
       " ',',\n",
       " 'so',\n",
       " 'i',\n",
       " 'usually',\n",
       " 'walk',\n",
       " 'by',\n",
       " 'the',\n",
       " 'river',\n",
       " 'bank',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'go',\n",
       " 'by',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'and',\n",
       " 'withdraw',\n",
       " 'some',\n",
       " 'money',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input['input_ids'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the tokenizer adds special tokens:\n",
    "- **[CLS]** (Classification): used as an aggregate representation of the entire sequence. Usually used in classification tasks\n",
    "- **[SEP]** (Seperator): Seperates parts of the input, here: the two sentences. Indicates the end of a sequence\n",
    "- **[PAD]** (Padding): Added to pad a sequence to a fixed length. Used to bring all input sequences to the same length for technical reasons. Note how the shorter of the two sentences get a [PAD]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the actual embeddings, we have to run the input into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad(): # torch.no_grad() drops the gradient tracking we'd only need when fine-tuning\n",
    "    embeddings = model(**input) # this returns the embeddings for each input sequence\n",
    "\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings consist of two outputs: \n",
    "\n",
    "1) The last **hidden states** for each of the input sequences (in our case each of the two sentences), which are the embeddings after passing all the attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acces the first sentence's embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0853, -0.0994, -0.3272,  ..., -0.1593,  0.2339,  0.5785],\n",
       "        [ 0.4413, -0.0587, -0.3399,  ..., -0.1189,  0.7490,  0.2191],\n",
       "        [ 0.0994,  0.6217,  0.7028,  ...,  0.3706,  0.1747,  0.4839],\n",
       "        ...,\n",
       "        [ 0.3449, -0.3056,  0.4267,  ..., -0.2619, -0.5350,  0.3470],\n",
       "        [ 0.5137, -0.0205, -0.3344,  ...,  0.4256, -0.3418, -0.4232],\n",
       "        [-0.2223, -0.2011, -0.1051,  ...,  0.3117,  0.3677, -0.3390]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.last_hidden_state[0] # this is identical to embeddings[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The so-called **pooler output**, which is a combined (pooled) representation of the entire input, and usually only used for tasks such as classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8573, -0.5435, -0.9416,  ..., -0.8946, -0.7382,  0.9487],\n",
       "        [-0.7943, -0.3355, -0.7300,  ..., -0.3702, -0.6826,  0.9381]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the embeddings for each of the input sentences, we can see that they are indeed quite different for each of the sentences. Notice the different axis ranges as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for i, emb in enumerate(embeddings.last_hidden_state):\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(emb)\n",
    "\n",
    "    plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1])\n",
    "\n",
    "    labels = tokenizer.convert_ids_to_tokens(input['input_ids'][i])\n",
    "    for j, txt in enumerate(labels):\n",
    "        color = \"red\" if \"bank\" in txt else \"black\"\n",
    "        plt.annotate(txt, (embeddings_2d[j, 0], embeddings_2d[j, 1]), color = color)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Just like with our static embeddings, our BERT model can have inherent bias due to the data it was trained on. However, as the embeddings are dependant on the context of a word, we cannnot simply check the vector similarity. Instead, we can mask a word in an input sequence and ask the model to return the most likely completion.\n",
    "\n",
    "Note that this is not a systematic evaluation of the model' biases, but it gives some idea how such an evaluation may be conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From v4.50 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# we need a specific model architecture for masked langauge modeling. we can re-use the tokenizer. \n",
    "# Note that we still use the same BERT model\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "mask_model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauke Licht wrote a handy function to get the top-k words for a masked token:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_topk_words(text):\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the index of the masked token\n",
    "    masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = mask_model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Get the log probabilities of the 10 best fitting words\n",
    "    log_probs = torch.log_softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_10_log_probs, top_10_indices = torch.topk(log_probs, 10)\n",
    "\n",
    "    # Convert indices to tokens\n",
    "    top_10_tokens = tokenizer.convert_ids_to_tokens(top_10_indices.tolist())\n",
    "\n",
    "    # Print the results\n",
    "    return pd.DataFrame({'token': top_10_tokens, 'log_prob': top_10_log_probs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>log_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>-1.800818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nice</td>\n",
       "      <td>-3.128358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cute</td>\n",
       "      <td>-3.288083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>-3.504081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>-3.606331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>close</td>\n",
       "      <td>-3.708217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hot</td>\n",
       "      <td>-3.829846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>strong</td>\n",
       "      <td>-3.862879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>excited</td>\n",
       "      <td>-4.041302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scared</td>\n",
       "      <td>-4.181984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  log_prob\n",
       "0       good -1.800818\n",
       "1       nice -3.128358\n",
       "2       cute -3.288083\n",
       "3  beautiful -3.504081\n",
       "4      happy -3.606331\n",
       "5      close -3.708217\n",
       "6        hot -3.829846\n",
       "7     strong -3.862879\n",
       "8    excited -4.041302\n",
       "9     scared -4.181984"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topk_words(\"He was really [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>log_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>-2.334454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>-2.970024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scared</td>\n",
       "      <td>-3.302579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excited</td>\n",
       "      <td>-3.344266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>-3.374403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pretty</td>\n",
       "      <td>-3.531833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>upset</td>\n",
       "      <td>-3.651014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nice</td>\n",
       "      <td>-3.709944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cute</td>\n",
       "      <td>-3.802214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tired</td>\n",
       "      <td>-3.873873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  log_prob\n",
       "0       good -2.334454\n",
       "1  beautiful -2.970024\n",
       "2     scared -3.302579\n",
       "3    excited -3.344266\n",
       "4      happy -3.374403\n",
       "5     pretty -3.531833\n",
       "6      upset -3.651014\n",
       "7       nice -3.709944\n",
       "8       cute -3.802214\n",
       "9      tired -3.873873"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topk_words(\"She was really [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "*Important note*: **The strength of the BERT model is not that it is great at any task - as LLMs try to be - but that it is relatively easy to fine-tune it to a specfic task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few pointers on classification tasks:\n",
    "\n",
    "1) **_Pick the right model for the task._** This means:\n",
    "\n",
    "    a) **Language**\n",
    "    Make sure the model \"speaks\" your language - that is, it was trained on sufficient data for the language you are looking to classify. A lot of smaller models (such as BERT models) are often only trained on English texts. So if you want to, for example, analyze German text data, pick a model that was trained or fine-tuned accordingly. While larger models are often multilingual (that is, trained with data from multiple languages), a smaller model specifically tuned for your input language might still perform better. This is especially  important for less-spoken languages which are more likely to be underrepresented in multilingual model. \n",
    "    \n",
    "    If you are unsure if the model is suitable for your input language, running a couple simple language understanding tasks such as predicting masked-out words can help to give a feeling for its capabilities.\n",
    "\n",
    "    b) **Task**\n",
    "    Depending on the complexity of your task, consider using a model specifically tuned for it. While tasks such as sentiment analysis are relatively common, other tasks - say, predicting policy categories - are less straight-forward and may require a more specialized model. Even for tasks such as sentiment analysis, you will need a model that has knowledge of the categories your are trying to predict. This is the case for most large language models, but for smaller models such as BERT, you will one that is tuned to your task. We will look into fine-tuning later, but you should know that *fine-tuning for a specific task often reduces performance in other tasks.*\n",
    "\n",
    "    You can find a lot of models for different languages and almost any task at the [Hugging Face model hub](https://huggingface.co/models). \n",
    "\n",
    "2) **_Evaluate your model output_**\n",
    "    *Looking at the first few results and deciding that it's good enough is not a proper evaluation*, even though it is often passed as such. In order to gauge if your model is capable of doing your task, you will most likely need to hand-code some of your samples. There are other approaches, such as using a more powerful LLM to judge the model output, but it is highly debatable if this is reliable. *Especially in social science research, you usually want to replicate the expert's - your! - judgement when classifying data!* \n",
    "    \n",
    "    The number of samples you need to code usually depends on the size of your data set and the number of categories you wish to encode. Also consider that human judgement may not be perfect, so you may want to employ multiple coders and evaluate the inter-coder reliability (more on this [here](https://doi.org/10.1080/10584609.2020.1723752)). *Evaluation is especially important when using the results in further analysis steps.* The r of your regression model is not worth much if the variable labels are only 60% accurate, now is it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment\n",
    "\n",
    "One of the most common tasks in NLP is sentiment classification. It predicts the sentiment of an input sequence and language models such as BERT are often used for it. However, our out-of-the-box BERT model will actually be quite bad at this, as it never learned what the sentiment labels (e.g. \"positive\" and \"negative\") mean. So we'll load up another model better suited for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"This thing is quite alright, but really not the best thing ever.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For common tasks such as classification, the transformer library comes with a handy shorthand to run the model for your tasks. They are called **pipelines** and take care of almost all the code usually required to run a task and get the results - including tokenization. Very handy!\n",
    "\n",
    "We can provide a model name to the pipeline. If the task the model was trained for is explicitly stated in the model, the pipeline will know what to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467ca0fd8ddd42b6aa8b210f1e8d4f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\nlp_workshop\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Tim\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e43e27ca6e4ce9a8f59fbce106e2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998866319656372},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997679591178894},\n",
       " {'label': 'NEGATIVE', 'score': 0.9982239603996277}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\") # this automatically loads the mode\n",
    "\n",
    "pipe(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you only define a task, e.g. sentiment analysis, the *pipeline* function will use a default model for the task.\n",
    "\n",
    "**Important:** The default model may not always be what you want, especially if your input is not in English (note how it defaults to an English model!). So it's always better to go to the [model hub](https://huggingface.co/models) and look for a suitable model there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998866319656372},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997679591178894},\n",
       " {'label': 'NEGATIVE', 'score': 0.9982239603996277}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_pipe = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def_pipe(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model does not explicitly define a task it was trained for, you may have to both specify the task and the model. For more information on pipelines, see: https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines\n",
    "\n",
    "Note that there are also pipelines readily available for data types other than text, such as images or audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the steps by yourself, rather than using the pipeline, you can do so with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEGATIVE': 0.00011340079072397202, 'POSITIVE': 0.9998866319656372}\n",
      "{'NEGATIVE': 0.9997679591178894, 'POSITIVE': 0.00023206845798995346}\n",
      "{'NEGATIVE': 0.9982239603996277, 'POSITIVE': 0.0017759873298928142}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# we need a different tokenizer for the new model\n",
    "sentiment_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# and load the model\n",
    "sentiment_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "inputs = sentiment_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = sentiment_model(**inputs).logits\n",
    "\n",
    "for i in range(logits.size(0)):\n",
    "    probs = torch.softmax(logits[i], dim=0).tolist()\n",
    "    probs = {sentiment_model.config.id2label[index]: probability for index, probability in enumerate(probs)}\n",
    "    print(probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "\n",
    "A more complex task is the classsification of texts into policy categories. A long-running project focused on this is the [Comparative Agendas Project (CAP)](https://www.comparativeagendas.net/). They have been coding different types of text into an elaborate coding scheme, with subprojects for many different countries running for decades. However, so far, they have been doing this qualitatively, with a large number of well-trained coders.\n",
    "\n",
    "The [poltextLAB](https://poltextlab.com/) has started to use the CAP's extensive collection of hand-coded policy and media texts to train classification models. They provide a vast collection of models for different tasks and languages. Apart from the CAP classification scheme, they also provide models to replicate the [Manifesto Project](https://manifesto-project.wzb.eu/) coding scheme for comparatively analyzing party manifestos, sentiment analysis and emotion analysis. You can find all their models on the model hub: https://huggingface.co/poltextlab\n",
    "\n",
    "*Note:* While the model use is free, you will need a Hugging Face account and provide the authors with some personal information in order to comply with their fair use guidelines.\n",
    "\n",
    "**Terminology:** *Multi-class classification* describes the task of sorting an object (here a text) into one of multiple classes (in our case, CAP policy categories). It is assumed that an object can only correctly be sorted into *one* class, not multiple classes. This replicates the CAP coding, as they also only assign one code to a given document. While it is debatable if this is realistic (and a text cannot actually touch in different policy subjects), the task of sorting an object into multiple categories is referred to as *multi-label classification*. It is, however, somewhat more complex and will not be covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to load the Hugging Face access token, we need to load the .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing large data sets, it can be beneficial to utilize your GPU (if available). The CUDA framework is most common and well-implemented, but it requires a NVIDIA GPU. For Mac devices, there is the MPS backend. If neither is available, you can always fall back to using your CPU. The following code automates this setup. By setting the correct 'device'-argument which you can then pass onto your `pipeline` function.\n",
    "\n",
    "*Note*: Utilizing CUDA requires a specific pytorch installation that comes shipped with CUDA. You can find more info on this here: https://pytorch.org/get-started/locally/. The terminal command `nvidia-smi` returns info on your GPU and, more importantly, the CUDA version you need to install with pytorch. You do *not* need to install CUDA yourself. Note that the CUDA-loaded pytorch installation can be rather large, 2.5gb in my case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Found. Running on NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "# device setup\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): # CUDA is the preferred device\n",
    "    device = 'cuda:0'\n",
    "    print(f'Cuda Found. Running on {torch.cuda.get_device_name(0)}')\n",
    "else: \n",
    "    if torch.backends.mps.is_available(): # MPS backend for Mac\n",
    "        device = 'mps'\n",
    "        print('MPS Found. Running on MPS')\n",
    "    else: \n",
    "        device = 'cpu' # fallback to CPU computing\n",
    "        print('No GPU Found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: The labels put out by the model are actually incorrect - they are off by one. Unfortunately, this is very badly documented at the model page. However, this becomes clear once you realize that the labels start at 0, but there is no category 0 in the CAP (999 is the \"other\" category).  Instead, the model classifies macroeconomic topics as LABEL_0, which are CAP category one. Also note that there is no category 11 (nor 22) in the CAP. See: https://www.comparativeagendas.net/pages/master-codebook. \n",
    "\n",
    "Therefore, the labels are off by 1, 2 (after 10) or 3 (for LABEL_20) and need to be corrected. We could do this on our labeled data after running the model. However, we can also edit the model's label-id-translation directly, so we do not have to replace the labels everytime we run the model. Note that the \"LABEL_\" output is only a label added after the model returns a number between 0 and 22 as classification. Therefore, we have to replace the labels assigned to these IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1',\n",
       " 1: '2',\n",
       " 2: '3',\n",
       " 3: '4',\n",
       " 4: '5',\n",
       " 5: '6',\n",
       " 6: '7',\n",
       " 7: '8',\n",
       " 8: '9',\n",
       " 9: '10',\n",
       " 10: '12',\n",
       " 11: '13',\n",
       " 12: '14',\n",
       " 13: '15',\n",
       " 14: '16',\n",
       " 15: '17',\n",
       " 16: '18',\n",
       " 17: '19',\n",
       " 18: '20',\n",
       " 19: '21',\n",
       " 20: '23'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "CAP_NUM_DICT = {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', # translates Model Output to CAP Major Policy Codes\n",
    "6: '7', 7: '8', 8: '9', 9: '10', 10: '12', 11: '13', 12: '14', \n",
    "13: '15', 14: '16', 15: '17', 16: '18', 17: '19', 18: '20', 19: \n",
    "'21', 20: '23',}\n",
    "\n",
    "# load the model\n",
    "cap_model = AutoModelForSequenceClassification.from_pretrained(\"poltextlab/xlm-roberta-large-english-media-cap-v3\")\n",
    "\n",
    "cap_model.config.id2label = CAP_NUM_DICT # replace the labels with the CAP Major Policy Codes\n",
    "cap_model.config.label2id = {value: key for key, value in CAP_NUM_DICT.items()} # same for the label2id (reversing the dictionary)\n",
    "\n",
    "cap_model.config.id2label # inspect the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass our adjusted model to the `pipeline` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "\n",
    "# we need to set the tokenizer, in this case the roberta model the classifier is based on\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\") \n",
    "\n",
    "classifier = pipeline(task='text-classification', # make classifier pipeline\n",
    "                      device=device, # this sets the device we prepared earlier\n",
    "                      model=cap_model, \n",
    "                      tokenizer = tokenizer, \n",
    "                      token=os.environ.get(\"HF_TOKEN\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify some data I've downloaded from https://www.comparativeagendas.net/datasets_codebooks\n",
    "\n",
    "We're using a data set on Times of London headlines from 1960 to 2008. Here's what it says on the CAP website about the data:\n",
    "\n",
    "    The UK media data measures the policy content of the front page of The Times of London. Front page news stories are sampled on the Wednesday of each week between 1960 and 2008, with the headlines blind-coded by two researchers according to major topic code, generating a database of 21,854 front page headlines. Due to important changes in the formatting of The Times during period between 1960 and 2008, along with a strike that stopped production for almost a year we strongly suggest that those interested in the media data refer to the dataset codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>speech_year</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>words</th>\n",
       "      <th>majortopic</th>\n",
       "      <th>filter_international</th>\n",
       "      <th>filter_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Engineering Unions Press Pay And Hours Claim</td>\n",
       "      <td>Apparition Of Inflationary Spiral Laid</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>In Disaster Pit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>796.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Peeping Toms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Police Officer Praised For Car Struggle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>540 Tugmen End Strike</td>\n",
       "      <td>Employers Agree To Supply Deck Boys</td>\n",
       "      <td>411.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  year  speech_year        date  \\\n",
       "0   1  1960           52  06/01/1960   \n",
       "1   2  1960           52  06/01/1960   \n",
       "2   3  1960           52  06/01/1960   \n",
       "3   4  1960           52  06/01/1960   \n",
       "4   5  1960           52  06/01/1960   \n",
       "\n",
       "                                    description  \\\n",
       "0  Engineering Unions Press Pay And Hours Claim   \n",
       "1                               In Disaster Pit   \n",
       "2                                  Peeping Toms   \n",
       "3       Police Officer Praised For Car Struggle   \n",
       "4                         540 Tugmen End Strike   \n",
       "\n",
       "                                 subtitle   words  majortopic  \\\n",
       "0  Apparition Of Inflationary Spiral Laid  1042.0           5   \n",
       "1                                     NaN   796.0           8   \n",
       "2                                     NaN   159.0          12   \n",
       "3                                     NaN   173.0          12   \n",
       "4     Employers Agree To Supply Deck Boys   411.0          10   \n",
       "\n",
       "   filter_international  filter_duplicate  \n",
       "0                     0                 0  \n",
       "1                     0                 0  \n",
       "2                     0                 0  \n",
       "3                     0                 0  \n",
       "4                     0                 0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "uk_media = pd.read_csv('data/uk_media.csv')\n",
    "\n",
    "uk_media.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the headline is actually labeled description. Let's change that, and combine headline and subtitle to give the model a better chance at recognizing the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>speech_year</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>words</th>\n",
       "      <th>majortopic</th>\n",
       "      <th>filter_international</th>\n",
       "      <th>filter_duplicate</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Engineering Unions Press Pay And Hours Claim</td>\n",
       "      <td>Apparition Of Inflationary Spiral Laid</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Engineering Unions Press Pay And Hours Claim A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>In Disaster Pit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>796.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In Disaster Pit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Peeping Toms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Peeping Toms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Police Officer Praised For Car Struggle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Police Officer Praised For Car Struggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>540 Tugmen End Strike</td>\n",
       "      <td>Employers Agree To Supply Deck Boys</td>\n",
       "      <td>411.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540 Tugmen End Strike Employers Agree To Suppl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21843</th>\n",
       "      <td>21844</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>State of paranoia all started with a coup - An...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>State of paranoia all started with a coup - An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21846</th>\n",
       "      <td>21847</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Tribute to Galileo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tribute to Galileo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21849</th>\n",
       "      <td>21850</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21850</th>\n",
       "      <td>21851</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21851</th>\n",
       "      <td>21852</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Clarification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Clarification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21699 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  year  speech_year        date  \\\n",
       "0          1  1960           52  06/01/1960   \n",
       "1          2  1960           52  06/01/1960   \n",
       "2          3  1960           52  06/01/1960   \n",
       "3          4  1960           52  06/01/1960   \n",
       "4          5  1960           52  06/01/1960   \n",
       "...      ...   ...          ...         ...   \n",
       "21843  21844  2008          102  31/12/2008   \n",
       "21846  21847  2008          102  31/12/2008   \n",
       "21849  21850  2008          102  31/12/2008   \n",
       "21850  21851  2008          102  31/12/2008   \n",
       "21851  21852  2008          102  31/12/2008   \n",
       "\n",
       "                                             description  \\\n",
       "0           Engineering Unions Press Pay And Hours Claim   \n",
       "1                                        In Disaster Pit   \n",
       "2                                           Peeping Toms   \n",
       "3                Police Officer Praised For Car Struggle   \n",
       "4                                  540 Tugmen End Strike   \n",
       "...                                                  ...   \n",
       "21843  State of paranoia all started with a coup - An...   \n",
       "21846                                 Tribute to Galileo   \n",
       "21849                                Dawn chorus dispute   \n",
       "21850                                Dawn chorus dispute   \n",
       "21851                                      Clarification   \n",
       "\n",
       "                                     subtitle   words  majortopic  \\\n",
       "0      Apparition Of Inflationary Spiral Laid  1042.0           5   \n",
       "1                                         NaN   796.0           8   \n",
       "2                                         NaN   159.0          12   \n",
       "3                                         NaN   173.0          12   \n",
       "4         Employers Agree To Supply Deck Boys   411.0          10   \n",
       "...                                       ...     ...         ...   \n",
       "21843                                     NaN   229.0          26   \n",
       "21846                                     NaN    62.0          28   \n",
       "21849                                     NaN    54.0          12   \n",
       "21850                                     NaN    51.0          12   \n",
       "21851                                     NaN    72.0          99   \n",
       "\n",
       "       filter_international  filter_duplicate  \\\n",
       "0                         0                 0   \n",
       "1                         0                 0   \n",
       "2                         0                 0   \n",
       "3                         0                 0   \n",
       "4                         0                 0   \n",
       "...                     ...               ...   \n",
       "21843                     1                 0   \n",
       "21846                     0                 0   \n",
       "21849                     0                 0   \n",
       "21850                     1                 0   \n",
       "21851                     0                 0   \n",
       "\n",
       "                                                    text  \n",
       "0      Engineering Unions Press Pay And Hours Claim A...  \n",
       "1                                       In Disaster Pit   \n",
       "2                                          Peeping Toms   \n",
       "3               Police Officer Praised For Car Struggle   \n",
       "4      540 Tugmen End Strike Employers Agree To Suppl...  \n",
       "...                                                  ...  \n",
       "21843  State of paranoia all started with a coup - An...  \n",
       "21846                                Tribute to Galileo   \n",
       "21849                               Dawn chorus dispute   \n",
       "21850                               Dawn chorus dispute   \n",
       "21851                                     Clarification   \n",
       "\n",
       "[21699 rows x 11 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll combine the two columns and call them \"text\"\n",
    "uk_media['text'] = uk_media['description'].fillna('') + ' ' + uk_media['subtitle'].fillna('') # fillna() makes sure missing values don't result in NaN entries\n",
    "\n",
    "# we'll also drop duplicates indicated by the filter_duplicate column\n",
    "uk_media = uk_media[uk_media['filter_duplicate'] == 0]\n",
    "\n",
    "# we'll also drop rows where text is NaN (missing due to missing headlines)\n",
    "uk_media = uk_media[uk_media['text'].notna()]\n",
    "\n",
    "uk_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 91,\n",
       " 99]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct majortopic labels, sorted\n",
    "\n",
    "sorted([int(value) for value in uk_media['majortopic'].unique()]) # the for loop is only here turn the values into native python integers, which print a bit nicer in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot a problem with our data set here: The coding seems to be different from the official CAP codes as well! A closer look at the codebooks reveals that codes over 23 are additional, media-specific and dummy codes which were not fed into the model. There is also an additional dummy code of 0 for non-codable articles. For evaluation purposes, we'll drop them.\n",
    "\n",
    "**You should always make sure your model is compatible with the categories you're using as gold standard!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with majortopic code above 23 and below 1\n",
    "uk_media = uk_media[uk_media['majortopic'] >= 1]\n",
    "uk_media = uk_media[uk_media['majortopic'] <= 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5', 'score': 0.936924159526825},\n",
       " {'label': '16', 'score': 0.2734913229942322},\n",
       " {'label': '12', 'score': 0.178557351231575},\n",
       " {'label': '12', 'score': 0.9686050415039062},\n",
       " {'label': '5', 'score': 0.8688399195671082},\n",
       " {'label': '12', 'score': 0.21066716313362122},\n",
       " {'label': '12', 'score': 0.924317479133606},\n",
       " {'label': '10', 'score': 0.9658780097961426},\n",
       " {'label': '10', 'score': 0.5852949619293213}]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier on the first 10 rows\n",
    "classifier(list(uk_media['text'][0:9])) # we need to pass the text column as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the whole dataset\n",
    "classifications = classifier(list(uk_media[\"text\"])) # we need to pass the text column as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as this may take a while we'll safe the classifications locally after the run\n",
    "# the pickle module can safe and load arbitrary Python objects\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/classifications.pkl', 'wb') as f:\n",
    "    pickle.dump(classifications, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then reload the \"pickle\" file when we need the classifcation results again later\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/classifications.pkl', 'rb') as f:\n",
    "    classifications = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>speech_year</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>words</th>\n",
       "      <th>majortopic</th>\n",
       "      <th>filter_international</th>\n",
       "      <th>filter_duplicate</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Engineering Unions Press Pay And Hours Claim</td>\n",
       "      <td>Apparition Of Inflationary Spiral Laid</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Engineering Unions Press Pay And Hours Claim A...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.936924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>In Disaster Pit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>796.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In Disaster Pit</td>\n",
       "      <td>16</td>\n",
       "      <td>0.273491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Peeping Toms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Peeping Toms</td>\n",
       "      <td>12</td>\n",
       "      <td>0.178557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>Police Officer Praised For Car Struggle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Police Officer Praised For Car Struggle</td>\n",
       "      <td>12</td>\n",
       "      <td>0.968605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "      <td>06/01/1960</td>\n",
       "      <td>540 Tugmen End Strike</td>\n",
       "      <td>Employers Agree To Supply Deck Boys</td>\n",
       "      <td>411.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540 Tugmen End Strike Employers Agree To Suppl...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.868840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17548</th>\n",
       "      <td>21836</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Murder charge boys</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Murder charge boys</td>\n",
       "      <td>12</td>\n",
       "      <td>0.981427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17549</th>\n",
       "      <td>21842</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Sir Tom pledges to return clothing chain to pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>379.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sir Tom pledges to return clothing chain to pr...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.924814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17550</th>\n",
       "      <td>21843</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>How small, local and traditional is bucking th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>813.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How small, local and traditional is bucking th...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.709025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17551</th>\n",
       "      <td>21850</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "      <td>3</td>\n",
       "      <td>0.617270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17552</th>\n",
       "      <td>21851</td>\n",
       "      <td>2008</td>\n",
       "      <td>102</td>\n",
       "      <td>31/12/2008</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Dawn chorus dispute</td>\n",
       "      <td>3</td>\n",
       "      <td>0.617270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17553 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  year  speech_year        date  \\\n",
       "0          1  1960           52  06/01/1960   \n",
       "1          2  1960           52  06/01/1960   \n",
       "2          3  1960           52  06/01/1960   \n",
       "3          4  1960           52  06/01/1960   \n",
       "4          5  1960           52  06/01/1960   \n",
       "...      ...   ...          ...         ...   \n",
       "17548  21836  2008          102  31/12/2008   \n",
       "17549  21842  2008          102  31/12/2008   \n",
       "17550  21843  2008          102  31/12/2008   \n",
       "17551  21850  2008          102  31/12/2008   \n",
       "17552  21851  2008          102  31/12/2008   \n",
       "\n",
       "                                             description  \\\n",
       "0           Engineering Unions Press Pay And Hours Claim   \n",
       "1                                        In Disaster Pit   \n",
       "2                                           Peeping Toms   \n",
       "3                Police Officer Praised For Car Struggle   \n",
       "4                                  540 Tugmen End Strike   \n",
       "...                                                  ...   \n",
       "17548                                 Murder charge boys   \n",
       "17549  Sir Tom pledges to return clothing chain to pr...   \n",
       "17550  How small, local and traditional is bucking th...   \n",
       "17551                                Dawn chorus dispute   \n",
       "17552                                Dawn chorus dispute   \n",
       "\n",
       "                                     subtitle   words majortopic  \\\n",
       "0      Apparition Of Inflationary Spiral Laid  1042.0          5   \n",
       "1                                         NaN   796.0          8   \n",
       "2                                         NaN   159.0         12   \n",
       "3                                         NaN   173.0         12   \n",
       "4         Employers Agree To Supply Deck Boys   411.0         10   \n",
       "...                                       ...     ...        ...   \n",
       "17548                                     NaN    64.0         12   \n",
       "17549                                     NaN   379.0         15   \n",
       "17550                                     NaN   813.0         15   \n",
       "17551                                     NaN    54.0         12   \n",
       "17552                                     NaN    51.0         12   \n",
       "\n",
       "       filter_international  filter_duplicate  \\\n",
       "0                         0                 0   \n",
       "1                         0                 0   \n",
       "2                         0                 0   \n",
       "3                         0                 0   \n",
       "4                         0                 0   \n",
       "...                     ...               ...   \n",
       "17548                     0                 0   \n",
       "17549                     0                 0   \n",
       "17550                     0                 0   \n",
       "17551                     0                 0   \n",
       "17552                     1                 0   \n",
       "\n",
       "                                                    text label     score  \n",
       "0      Engineering Unions Press Pay And Hours Claim A...     5  0.936924  \n",
       "1                                       In Disaster Pit     16  0.273491  \n",
       "2                                          Peeping Toms     12  0.178557  \n",
       "3               Police Officer Praised For Car Struggle     12  0.968605  \n",
       "4      540 Tugmen End Strike Employers Agree To Suppl...     5  0.868840  \n",
       "...                                                  ...   ...       ...  \n",
       "17548                                Murder charge boys     12  0.981427  \n",
       "17549  Sir Tom pledges to return clothing chain to pr...    15  0.924814  \n",
       "17550  How small, local and traditional is bucking th...    15  0.709025  \n",
       "17551                               Dawn chorus dispute      3  0.617270  \n",
       "17552                               Dawn chorus dispute      3  0.617270  \n",
       "\n",
       "[17553 rows x 13 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the classifications to our UK media dataframe\n",
    "\n",
    "classifications_df = pd.DataFrame(classifications) # turn the classifications a DataFrame\n",
    "\n",
    "uk_media.reset_index(drop=True, inplace=True) # we need to reset the indices to avoid mismatches\n",
    "classifications_df.reset_index(drop=True, inplace=True) #  (the classifications index is a fresh one while the uk_media carries missing indices where we removed the data before)\n",
    "\n",
    "uk_media_classified = pd.concat([uk_media, # combine the two DataFrames\n",
    "                                 classifications_df], \n",
    "                                 axis=1) \n",
    "\n",
    "# for evaluation purposes, we'll also turn the majortopic into strings\n",
    "uk_media_classified['majortopic'] = uk_media_classified['majortopic'].astype(str)\n",
    "\n",
    "uk_media_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Standard\n",
    "Let's take a look how well our model did. Luckily, the UK Media Dataset comes with labels annotated by trained human coders. This is often referred to as the *Gold Standard*, meaning it will be the standard our model will be held to. In other words, our model would be \"perfect\" if it could perfectly recreate the human coding. It is important to note that even another trained human coder is unlikely to recreate the exact same coding. This is due to differences and errors in human judgement. This is why in qualitative research, we use the measure of intercoder reliability to make sure our human coders have substandial agreement on their coding tasks. There are some debates on the matter (see, for example, [this paper](https://doi.org/10.1177/1609406919899220)) and various way to calculate it (with Krippendorf's Alpha being the most common), but usually you want your intercoder reliability to be above 85%, with values above 65% sometimes being considered acceptable, depending on the task and data. For our model, this means that we can usually accept it if it does not reproduce our gold standard to 100% (sometimes, this can even be a sign of overfitting to the data). \n",
    "\n",
    "However, while some researchers argue otherwise, I strongly recommend against using intercoder reliability to judge model output. The main reason for this is that it somewhat relaxes assumptions by calculating the measurement in such a way that the model is considered equally capable of coding the data as a trained human coder. This is simply not the case, as we usually want to find out whether or not our model can meaningfully replicate human coders' judgement. Intercoder reliability is an important tool when producing the gold standard, as it makes sure it is actually a *good* standard, and you should always make use of it when qualitatively coding a data set (even when coding it by yourself, it is good practice to double code a part of your data to make sure your coding is reliable!). But for measuring a model's capabilities, it is strongly adviced to rely on the evaluation metrics developed in the machine learning domain to measure exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is a very simple measure: it is the percentage of correct predictions out of all predictions made by the model. As a formula: \n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "While this is an important metric, it is often not a very good metric if your classes are imbalanced. Accuracy puts higher emphasis on the majority class, which is often not the one we're interested in. Think of a spam filter which is great at predicting which emails are not spam, but fails at predicting the (much rarer case) of an email actually being spam. This is especially a problem in multi-class classification, as your less-well represented classes are not only harder to predict for the model due to a lack of training data, but its failure to predict them will also not show in your evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F1\n",
    "To overcome these limitations, precision, recall and F1 scores were developed. They rely on the differentiation between True and False Positives and True and False Negatives.\n",
    "\n",
    "| <a/> | **Predicted Positive** | **Predicted Negative** |\n",
    "|:--- |:---:|:---:|\n",
    "| **Observed Positive** | True Positives (TP) | False Negatives (FN) |\n",
    "| **Observed Negative** | False Positives (FP) | True Negatives (TN) |\n",
    "\n",
    "- **True Positives (TP)**: The model correctly predicted the positive class.\n",
    "- **True Negatives (TN)**: The model correctly predicted the negative class.\n",
    "- **False Positives (FP)**: The model incorrectly predicted the positive class (predicted positive, but the true label was negative).\n",
    "- **False Negatives (FN)**: The model incorrectly predicted the negative class (predicted negative, but the true label was positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision measures the proportion of True Positives out of all positive predictions made by the model:\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "That is, it measures how many positive predictions were actually correct. Precision is the preferred metric if the cost of false positives is high. For example, if your spam filter automatically deletes any email it identifies as spam, you will want very high precision so it does not accidentally delete your other emails (false positives).\n",
    "\n",
    "However, the problem with precision is that conservative estimates fare better, as they produce less false positives. If your spam filter is not flagging a lot of mails as spam to begin with, it will have high precision, but be also not very good at its job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall measures the proportion of True Positives correctly identified by the model:\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "Recall is the preferred metric when the cost of false negatives is high, for example in medical tests or in flagging hate speech detection where you want to err on the side of caution. \n",
    "\n",
    "The problem with recall, is opposed to that of precision: you can achieve perfect recall by predicting all labels as positive, thus making sure you do not miss any relevant cases.\n",
    "\n",
    "Recall is sometimes also called sensitivity - especially in the medical field. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score is the harmoic mean of precision and recall. That is, it gives an even measure for precision and recall, balancing out their respective weaknesses.\n",
    "\n",
    "$$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "It is usually a good overview score of how well your model fares. If you do not have specific requirements for your model that make you value either precision or recall more highly (that is, if neither the cost of false negatives nor of false positives is higher), you will usually want to go with the F1 score to judge model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "In order to give you a feeling what these values mean: If we want to classify into one of two categories, using a coin flip for a classifier would produce an accuracy, precision, recall and F1 score of 0.5 - that is, there is a 50% chance of predicting the right class. So if your evaluation metrics are below 0.5, you would be better off just flipping a coin. If you have four classes, random chance of predicting the right one would be 25%, and the scores would equally come down to 0.25. \n",
    "\n",
    "So you should keep in mind that what is a \"good enough\" also depends on the number of categories you're trying to predict. However, eventually it depends on your use-case what the critical value for a reliable model is. For social science research, a rule of thumb is that you want your model to be roughly as reliable as human coders, which means values above 0.85 are considered good, but lower values may be acceptable in some cases. But again, this depends on your specific use case and you should think about what it means when, say, your measurements are only correct in two out of three cases (accuracy of 0.65) - and maybe even worse for some less well-represented categories. Can you make meaningful assumptions at this point? Or should you consider using a different method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "While we could calculate all these scores by hand, the scikit-learn library luckily has handy functions to do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\nlp_workshop\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7516094114966103\n",
      "Precision: 0.7720333797188088\n",
      "Recall: 0.7516094114966103\n",
      "F1: 0.7549872044599232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"])\n",
    "precision = precision_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"], average=\"weighted\")\n",
    "recall = recall_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"], average=\"weighted\")\n",
    "f1 = f1_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"], average=\"weighted\")\n",
    "\n",
    "print(\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'Recall: {recall}\\n'\n",
    "    f'F1: {f1}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We even have a handy overview function for all of this together, and print metrics over all classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.84      0.72      0.77       784\n",
      "          10       0.88      0.88      0.88      1747\n",
      "          12       0.87      0.79      0.83      2507\n",
      "          13       0.76      0.38      0.51       133\n",
      "          14       0.80      0.74      0.77       318\n",
      "          15       0.76      0.55      0.64       871\n",
      "          16       0.79      0.69      0.74      2568\n",
      "          17       0.87      0.71      0.78       801\n",
      "          18       0.74      0.76      0.75       113\n",
      "          19       0.61      0.81      0.70      2526\n",
      "           2       0.51      0.53      0.52       139\n",
      "          20       0.58      0.77      0.66      1328\n",
      "          21       0.70      0.52      0.59       419\n",
      "          22       0.00      0.00      0.00         2\n",
      "          23       0.02      0.67      0.03         3\n",
      "           3       0.87      0.80      0.84       795\n",
      "           4       0.84      0.60      0.70       331\n",
      "           5       0.72      0.77      0.74       688\n",
      "           6       0.83      0.93      0.88       619\n",
      "           7       0.62      0.68      0.65       240\n",
      "           8       0.89      0.74      0.81       536\n",
      "           9       0.74      0.61      0.67        82\n",
      "\n",
      "    accuracy                           0.75     17553\n",
      "   macro avg       0.66      0.64      0.63     17553\n",
      "weighted avg       0.77      0.75      0.75     17553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\nlp_workshop\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\miniconda3\\envs\\nlp_workshop\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\miniconda3\\envs\\nlp_workshop\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
