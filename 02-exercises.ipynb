{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Exercises\n",
    "\n",
    "Now you have the time to play around with classifiers yourself. We'll use a model for classifying German Party Manifesto contents, and run it on some data on German party manifestos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is `xlm-roberta-large-german-party-cap-v3` by poltextLAB. You should have requested access to it beforehand.\n",
    "\n",
    "The data we'll be using is coded CAP data on German party manifestos. You can download it from [https://www.comparativeagendas.net/project/germany/datasets](https://www.comparativeagendas.net/project/germany/datasets), but you will also find it in the data folder.\n",
    "\n",
    "**If you do not have access to the poltextLAB model**, search the [Hugging Face Hub](https://huggingface.co/models) for a model suitable to classify the data. It can be anything - a classifier for sentiment, topics, etc. To start with, I'd suggest that you look for a sentiment classifier. Just make sure it is suitable for German texts! Also be aware that, since we do not have a gold standard for this classifier, you will not be able to systematically evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the data under `data/manifesto_1949-2013_website-release_2.5_2_3.csv`. Take a look under \"Political Parties\" at https://www.comparativeagendas.net/project/germany/datasets to learn more about the data set.\n",
    "\n",
    "*Note*: The data is classified into both CAP major topics and subtopics, but the model only classifies into major topics (max 2 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "Load the Model from `poltextlab/xlm-roberta-large-german-party-cap-v3`. \n",
    "\n",
    "Check the model page and make sure the CAP code and the model output align. You can chekc the model output labels with the `config.id2label` attribute of your model. You can also use this (and `config.label2id`) to change the labeling.\n",
    "\n",
    "Build a classifier by using the `pipeline` function. Remember to pass your Hugging Face API token, the tokenizer, and the device if using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model\n",
    "\n",
    "Classify your data. As this may take a while, it is good practice to first run it on a small sample (e.g. 10 rows of your data) to make sure it works, and save the output after you ran your whole dataset.\n",
    "\n",
    "If you do not want to wait for the whole data to be processed by the model, you can also just run it on a subset. In this case, you can set a seed and use `sample()` to draw a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "**If not using the poltextLAB model, systematic evaluation will not be possible.** In this case, I suggest you look at a number of results to get a feeling for how well your model did. If you wanted to do a proper evaluation, you would need to manually code a percentage of your data and compare it to your model output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Data compability\n",
    "\n",
    "Check if your data and model have the same coded CAP categories. If the data has categories not available to the model, you might want to make a new dataset that drops these before calulating the evaluation metrics. If this is the case, take a closer look at the categories also: Which categories did the model predict for the categories it did not know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Calculate the Accuracy, Precision, Recall and F1 Score both across all categories and for each category individually. Are there any categories that fare better or worse than the rest? Why could that be?\n",
    "\n",
    "Also take a closer look at the additional variables in our data. Are the evaluation metrics dependant on the year or the party? Maybe the model struggles for certain periods or parties? Visualize this with plots.\n",
    "\n",
    "Hint: You might need to turn the CAP-labelling into string with `astype(str)` to calculate metrics.\n",
    "\n",
    "Note: If you pulled a random sample with `sample()`, you'll want to sort your data by year with `sort_values()` before plotting over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: New Data\n",
    "\n",
    "As mentioned before, it is likely that this specific data set has been used for training the model. Can you find some other data to run the model on? Maybe from your own research, or a paper that has made its data available?\n",
    "\n",
    "If you do not wish to work with German-language data, you might need to use another model, such as the English media model used in the previous notebook.\n",
    "\n",
    "Note that you will likely not have a gold standard for this data that you can directly evaluate the model output with (unless, of course, you have some CAP-coded data). In this case, you can still inspect the results to get a feeling how well the model did. A proper evaluation would require you to CAP-code the data, though!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
