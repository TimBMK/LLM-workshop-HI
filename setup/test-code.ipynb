{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "Are the requrired packages up and running? If not, install them as described in the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import seaborn\n",
    "import nltk\n",
    "import gensim\n",
    "import transformers\n",
    "import dotenv\n",
    "import langchain\n",
    "import langchain_ollama\n",
    "import langchain_openai\n",
    "import langchain_chroma\n",
    "import langchain_community\n",
    "import utils\n",
    "import langgraph\n",
    "import langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs\n",
    "\n",
    "Make sure you saved your API key in the .env file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "\n",
    "Check if you have access to the required models. Note that this will download the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should give you a sentiment classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillbert = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "distillbert(\"This workshop will be great!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should give you similar output, but with different labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\") \n",
    "\n",
    "cap_en = pipeline(task='text-classification', # make classifier pipeline\n",
    "                      model=\"poltextlab/xlm-roberta-large-english-media-cap-v3\", \n",
    "                      tokenizer=tokenizer,\n",
    "                      token=os.environ.get(\"HF_TOKEN\")) \n",
    "\n",
    "print(cap_en(\"This is a policy-related text in the media.\"))\n",
    "\n",
    "cap_de = pipeline(task='text-classification', # make classifier pipeline\n",
    "                      model=\"poltextlab/xlm-roberta-large-german-party-cap-v3\", \n",
    "                      tokenizer=tokenizer,\n",
    "                      token=os.environ.get(\"HF_TOKEN\")) \n",
    "\n",
    "print(cap_de(\"Das ist ein Parteiprogramm.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "Check if your OpenAI key works. This code should give you an API response. The `message.content` should read *positive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) # client = communicates with API\n",
    "\n",
    "instructions = \"\"\" \n",
    "You are a sentiment classification system. \n",
    "\n",
    "You will be given a sentence and you have to determine if the sentence is \"positive\", \"neutral\", or \"negative\".\n",
    "\n",
    "Only reply with \"positive\", \"neutral\", or \"negative\".\n",
    "\"\"\" # the triple quotes allow us to have a multi-line string\n",
    "\n",
    "messages = [\n",
    "    # we pass the instructions as system message\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": instructions\n",
    "    },\n",
    "    # and the content we wish to classify as user message\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I love this movie!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create( # completions = create responses\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Check if Ollama is installed. If you get a Warning about not connecting to a running Ollama instance, this is okay. Client version should be 0.5.4 (or higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "\n",
    "**Optional**. This checks if your CUDA device is recognized. If not, make sure you installed the correct torch version shipped with CUDA (see setup). If you do not have a CUDA device (usually a NVIDIA GPU) on your machine, you can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): # CUDA is the preferred device\n",
    "    device = 'cuda:0' # note that if you have multiple GPUs, you might want to change this value to not use the first one\n",
    "    print(f'Cuda Found. Running on {torch.cuda.get_device_name(0)}')\n",
    "else: \n",
    "    if torch.backends.mps.is_available(): # MPS backend for Mac\n",
    "        device = 'mps'\n",
    "        print('MPS Found. Running on MPS')\n",
    "    else: \n",
    "        device = 'cpu' # fallback to CPU computing\n",
    "        print('No GPU Found. Running on CPU')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
