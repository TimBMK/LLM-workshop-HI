{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomer Models\n",
    "\n",
    "This notebook uses the *transformers* library. It is a unified framework for deploying models and developed by [huggingface](https://huggingface.co/). Among other things, it provides functionality to download, train and run language models. You can find a list of models in the [model hub](https://huggingface.co/models). Note that the model hub also contains large language models often not suitable for local deployment. Instead, it has become common to run LLMs remotely via APIs. We will look into frameworks for this later.\n",
    "\n",
    "The *transformers* library is commonly used to work with models deployed locally. While this means that we can often not run Large Language Models on consumer-grade hardware, it is perfectly suitable for smaller models such as BERT, which are already quite powerful and perectly suited for a lot of tasks - especially when fine-tuned. If you run the *transformers* library on a more powerful server (e.g. AWS, Google or Azure), however, it allows you to run very large models. We will look into deploying quantized LLMs, which circumvent some of these limitations, later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# define the model name here for convenience\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "# load model and tokenizer. downloads the model if necessary\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *tokenizer* translates the words of a given input to the numbers associated with it by the model. This is necessary, as the actual model only deals in numbers - not text input. This is basically the same mechanism as the `index_to_key` method in our static embeddings before. \n",
    "\n",
    "**Important**: tokenizer and model need to match, otherwise the model does not associate the right embeddings with the words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the model. Notice how it differentiates between the static word embeddings, additional  embeddings (such as the position in the sequence), and the Encoder. The Attention layer in the Encoder here has 12 stacked  (\"BertLayers\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static word embeddings here have a vocabulary size of 30.522 over 768 embedding dimensions. While it may seem counter-intuitive that BERT has a smaller vocabulary size than the less powerful word2vec model, this can be easily explained: In BERT (and similar models), words are broken town into subwords before embedding. This makes the embedding more flexible and performant, and gives it a better chance at recognizing out-of-vocabulary words!\n",
    "\n",
    "For example, In Word2Vec, the words \"running\", \"runner\", and \"runs\" would each be separate tokens.\n",
    "In BERT, these words might be tokenized into subwords like \"run\", \"##ning\", \"##ner\", and \"##s\", where \"##\" indicates a subword that is part of a larger word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualized Embeddings\n",
    "\n",
    "Let's see how the BERT model has different embeddings for the same word (\"bank\") in different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I like water, so I usually walk by the river bank.\",\n",
    "             \"I have to go by the bank and withdraw some money.\"\n",
    "]\n",
    "\n",
    "# we need to tokenzie the sentences before feeding them to the \n",
    "\n",
    "input = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: A \"tensor\" object is something like a multidimensional matrix. While this is not technically correct, it is mostly used this way in data science. If you want to find out what a tensor actually is, I recommend reading up on linear algebra, or ask somebody knowledgable, like a physicist or mathematician. But be aware that physicists and mathematicians might disagree on the nature of a tensor...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the words in the sequence, we need to convert the IDs back to tokens. Note how the tokenizer added special tokens in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(input['input_ids'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the tokenizer adds special tokens:\n",
    "- **[CLS]** (Classification): used as an aggregate representation of the entire sequence. Usually used in classification tasks\n",
    "- **[SEP]** (Seperator): Seperates parts of the input, here: the two sentences. Indicates the end of a sequence\n",
    "- **[PAD]** (Padding): Added to pad a sequence to a fixed length. Used to bring all input sequences to the same length for technical reasons. Note how the shorter of the two sentences get a [PAD]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the actual embeddings, we have to run the input into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad(): # torch.no_grad() drops the gradient tracking we'd only need when fine-tuning\n",
    "    embeddings = model(**input) # this returns the embeddings for each input sequence\n",
    "\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings consist of two outputs: \n",
    "\n",
    "1) The last **hidden states** for each of the input sequences (in our case each of the two sentences), which are the embeddings after passing all the attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acces the first sentence's embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.last_hidden_state[0] # this is identical to embeddings[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The so-called **pooler output**, which is a combined (pooled) representation of the entire input, and usually only used for tasks such as classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the embeddings for each of the input sentences, we can see that they are indeed quite different for each of the sentences. Notice the different axis ranges as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for i, emb in enumerate(embeddings.last_hidden_state):\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(emb)\n",
    "\n",
    "    plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1])\n",
    "\n",
    "    labels = tokenizer.convert_ids_to_tokens(input['input_ids'][i])\n",
    "    for j, txt in enumerate(labels):\n",
    "        color = \"red\" if \"bank\" in txt else \"black\"\n",
    "        plt.annotate(txt, (embeddings_2d[j, 0], embeddings_2d[j, 1]), color = color)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Just like with our static embeddings, our BERT model can have inherent bias due to the data it was trained on. However, as the embeddings are dependant on the context of a word, we cannnot simply check the vector similarity. Instead, we can mask a word in an input sequence and ask the model to return the most likely completion.\n",
    "\n",
    "Note that this is not a systematic evaluation of the model' biases, but it gives some idea how such an evaluation may be conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a specific model architecture for masked langauge modeling. we can re-use the tokenizer. \n",
    "# Note that we still use the same BERT model\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "mask_model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauke Licht wrote a handy function to get the top-k words for a masked token:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_topk_words(text):\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the index of the masked token\n",
    "    masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = mask_model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Get the log probabilities of the 10 best fitting words\n",
    "    log_probs = torch.log_softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_10_log_probs, top_10_indices = torch.topk(log_probs, 10)\n",
    "\n",
    "    # Convert indices to tokens\n",
    "    top_10_tokens = tokenizer.convert_ids_to_tokens(top_10_indices.tolist())\n",
    "\n",
    "    # Print the results\n",
    "    return pd.DataFrame({'token': top_10_tokens, 'log_prob': top_10_log_probs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topk_words(\"He was really [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topk_words(\"She was really [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "*Important note*: **The strength of the BERT model is not that it is great at any task - as LLMs try to be - but that it is relatively easy to fine-tune it to a specfic task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few pointers on classification tasks:\n",
    "\n",
    "1) **_Pick the right model for the task._** This means:\n",
    "\n",
    "    a) **Language**\n",
    "    Make sure the model \"speaks\" your language - that is, it was trained on sufficient data for the language you are looking to classify. A lot of smaller models (such as BERT models) are often only trained on English texts. So if you want to, for example, analyze German text data, pick a model that was trained or fine-tuned accordingly. While larger models are often multilingual (that is, trained with data from multiple languages), a smaller model specifically tuned for your input language might still perform better. This is especially  important for less-spoken languages which are more likely to be underrepresented in multilingual model. \n",
    "    \n",
    "    If you are unsure if the model is suitable for your input language, running a couple simple language understanding tasks such as predicting masked-out words can help to give a feeling for its capabilities.\n",
    "\n",
    "    b) **Task**\n",
    "    Depending on the complexity of your task, consider using a model specifically tuned for it. While tasks such as sentiment analysis are relatively common, other tasks - say, predicting policy categories - are less straight-forward and may require a more specialized model. Even for tasks such as sentiment analysis, you will need a model that has knowledge of the categories your are trying to predict. This is the case for most large language models, but for smaller models such as BERT, you will one that is tuned to your task. We will look into fine-tuning later, but you should know that *fine-tuning for a specific task often reduces performance in other tasks.*\n",
    "\n",
    "    You can find a lot of models for different languages and almost any task at the [Hugging Face model hub](https://huggingface.co/models). \n",
    "\n",
    "2) **_Evaluate your model output_**\n",
    "    *Looking at the first few results and deciding that it's good enough is not a proper evaluation*, even though it is often passed as such. In order to gauge if your model is capable of doing your task, you will most likely need to hand-code some of your samples. There are other approaches, such as using a more powerful LLM to judge the model output, but it is highly debatable if this is reliable. *Especially in social science research, you usually want to replicate the expert's - your! - judgement when classifying data!* \n",
    "    \n",
    "    The number of samples you need to code usually depends on the size of your data set and the number of categories you wish to encode. Also consider that human judgement may not be perfect, so you may want to employ multiple coders and evaluate the inter-coder reliability (more on this [here](https://doi.org/10.1080/10584609.2020.1723752)). *Evaluation is especially important when using the results in further analysis steps.* The rÂ² of your regression model is not worth much if the variable labels are only 60% accurate, now is it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment\n",
    "\n",
    "One of the most common tasks in NLP is sentiment classification. It predicts the sentiment of an input sequence and language models such as BERT are often used for it. However, our out-of-the-box BERT model will actually be quite bad at this, as it never learned what the sentiment labels (e.g. \"positive\" and \"negative\") mean. So we'll load up another model better suited for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"This thing is quite alright, but really not the best thing ever.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For common tasks such as classification, the transformer library comes with a handy shorthand to run the model for your tasks. They are called **pipelines** and take care of almost all the code usually required to run a task and get the results - including tokenization. Very handy!\n",
    "\n",
    "We can provide a model name to the pipeline. If the task the model was trained for is explicitly stated in the model, the pipeline will know what to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\") # this automatically loads the mode\n",
    "\n",
    "pipe(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you only define a task, e.g. sentiment analysis, the *pipeline* function will use a default model for the task.\n",
    "\n",
    "**Important:** The default model may not always be what you want, especially if your input is not in English (note how it defaults to an English model!). So it's always better to go to the [model hub](https://huggingface.co/models) and look for a suitable model there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_pipe = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def_pipe(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model does not explicitly define a task it was trained for, you may have to both specify the task and the model. For more information on pipelines, see: https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines\n",
    "\n",
    "Note that there are also pipelines readily available for data types other than text, such as images or audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the steps by yourself, rather than using the pipeline, you can do so with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# we need a different tokenizer for the new model\n",
    "sentiment_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# and load the model\n",
    "sentiment_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "inputs = sentiment_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = sentiment_model(**inputs).logits\n",
    "\n",
    "for i in range(logits.size(0)):\n",
    "    probs = torch.softmax(logits[i], dim=0).tolist()\n",
    "    probs = {sentiment_model.config.id2label[index]: probability for index, probability in enumerate(probs)}\n",
    "    print(probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "\n",
    "A more complex task is the classsification of texts into policy categories. A long-running project focused on this is the [Comparative Agendas Project (CAP)](https://www.comparativeagendas.net/). They have been coding different types of text into an elaborate coding scheme, with subprojects for many different countries running for decades. However, so far, they have been doing this qualitatively, with a large number of well-trained coders.\n",
    "\n",
    "The [poltextLAB](https://poltextlab.com/) has started to use the CAP's extensive collection of hand-coded policy and media texts to train classification models. They provide a vast collection of models for different tasks and languages. Apart from the CAP classification scheme, they also provide models to replicate the [Manifesto Project](https://manifesto-project.wzb.eu/) coding scheme for comparatively analyzing party manifestos, sentiment analysis and emotion analysis. You can find all their models on the model hub: https://huggingface.co/poltextlab\n",
    "\n",
    "*Note:* While the model use is free, you will need a Hugging Face account and provide the authors with some personal information in order to comply with their fair use guidelines.\n",
    "\n",
    "**Terminology:** *Multi-class classification* describes the task of sorting an object (here a text) into one of multiple classes (in our case, CAP policy categories). It is assumed that an object can only correctly be sorted into *one* class, not multiple classes. This replicates the CAP coding, as they also only assign one code to a given document. While it is debatable if this is realistic (and a text cannot actually touch in different policy subjects), the task of sorting an object into multiple categories is referred to as *multi-label classification*. It is, however, somewhat more complex and will not be covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to load the Hugging Face access token, we need to load the .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing large data sets, it can be beneficial to utilize your GPU (if available). The CUDA framework is most common and well-implemented, but it requires a NVIDIA GPU. For Mac devices, there is the MPS backend. If neither is available, you can always fall back to using your CPU. The following code automates this setup. By setting the correct 'device'-argument which you can then pass onto your `pipeline` function.\n",
    "\n",
    "*Note*: Utilizing CUDA requires a specific pytorch installation that comes shipped with CUDA. You can find more info on this here: https://pytorch.org/get-started/locally/. The terminal command `nvidia-smi` returns info on your GPU and, more importantly, the CUDA version you need to install with pytorch. You do *not* need to install CUDA yourself. Note that the CUDA-loaded pytorch installation can be rather large, 2.5gb in my case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setup\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): # CUDA is the preferred device\n",
    "    device = 'cuda:0' # note that if you have multiple GPUs, you might want to change this value to not use the first one\n",
    "    print(f'Cuda Found. Running on {torch.cuda.get_device_name(0)}')\n",
    "else: \n",
    "    if torch.backends.mps.is_available(): # MPS backend for Mac\n",
    "        device = 'mps'\n",
    "        print('MPS Found. Running on MPS')\n",
    "    else: \n",
    "        device = 'cpu' # fallback to CPU computing\n",
    "        print('No GPU Found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: The labels put out by the model are actually incorrect - they are off by one. Unfortunately, this is not ver intuitive, even though the model page notes this now. It also becomes clear once you realize that the labels start at 0, but there is no category 0 in the CAP (999 is the \"other\" category).  Instead, the model classifies macroeconomic topics as LABEL_0, which are CAP category one. Also note that there is no category 11 (nor 22) in the CAP. See: https://www.comparativeagendas.net/pages/master-codebook. \n",
    "\n",
    "Therefore, the labels are off by 1, 2 (after 10) or 3 (for LABEL_20) and need to be corrected. We could do this on our labeled data after running the model. However, we can also edit the model's label-id-translation directly, so we do not have to replace the labels everytime we run the model. Note that the \"LABEL_\" output is only a label added after the model returns a number between 0 and 22 as classification. Therefore, we have to replace the labels assigned to these IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# the translation table is now also available on the model page and can be copied from there\n",
    "CAP_NUM_DICT = {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', # translates Model Output to CAP Major Policy Codes\n",
    "6: '7', 7: '8', 8: '9', 9: '10', 10: '12', 11: '13', 12: '14', \n",
    "13: '15', 14: '16', 15: '17', 16: '18', 17: '19', 18: '20', 19: \n",
    "'21', 20: '23', 21: '999'}\n",
    "\n",
    "# load the model\n",
    "cap_model = AutoModelForSequenceClassification.from_pretrained(\"poltextlab/xlm-roberta-large-english-media-cap-v3\")\n",
    "\n",
    "cap_model.config.id2label = CAP_NUM_DICT # replace the labels with the CAP Major Policy Codes\n",
    "cap_model.config.label2id = {value: key for key, value in CAP_NUM_DICT.items()} # same for the label2id (reversing the dictionary)\n",
    "\n",
    "cap_model.config.id2label # inspect the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass our adjusted model to the `pipeline` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "\n",
    "# we need to set the tokenizer, in this case the roberta model the classifier is based on\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\") \n",
    "\n",
    "classifier = pipeline(task='text-classification', # make classifier pipeline\n",
    "                      device=device, # this sets the device we prepared earlier\n",
    "                      model=cap_model, \n",
    "                      tokenizer = tokenizer, \n",
    "                      token=os.environ.get(\"HF_TOKEN\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify some data I've downloaded from https://www.comparativeagendas.net/datasets_codebooks\n",
    "\n",
    "We're using a data set on Times of London headlines from 1960 to 2008. Here's what it says on the CAP website about the data:\n",
    "\n",
    "    The UK media data measures the policy content of the front page of The Times of London. Front page news stories are sampled on the Wednesday of each week between 1960 and 2008, with the headlines blind-coded by two researchers according to major topic code, generating a database of 21,854 front page headlines. Due to important changes in the formatting of The Times during period between 1960 and 2008, along with a strike that stopped production for almost a year we strongly suggest that those interested in the media data refer to the dataset codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "uk_media = pd.read_csv('data/uk_media.csv')\n",
    "\n",
    "uk_media.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the headline is actually labeled description. Let's change that, and combine headline and subtitle to give the model a better chance at recognizing the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll combine the two columns and call them \"text\"\n",
    "uk_media['text'] = uk_media['description'].fillna('') + ' ' + uk_media['subtitle'].fillna('') # fillna() makes sure missing values don't result in NaN entries\n",
    "\n",
    "# we'll also drop duplicates indicated by the filter_duplicate column\n",
    "uk_media = uk_media[uk_media['filter_duplicate'] == 0]\n",
    "\n",
    "# we'll also drop rows where text is NaN (missing due to missing headlines)\n",
    "uk_media = uk_media[uk_media['text'].notna()]\n",
    "\n",
    "uk_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct majortopic labels, sorted\n",
    "\n",
    "sorted([int(value) for value in uk_media['majortopic'].unique()]) # the for loop is only here turn the values into native python integers, which print a bit nicer in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot a problem with our data set here: The coding seems to be different from the official CAP codes as well! A closer look at the codebooks reveals that codes over 23 are additional, media-specific codes which were not fed into the model. There is also an additional dummy code of 0 for non-codable articles. For evaluation purposes, we'll drop them.\n",
    "\n",
    "**You should always make sure your model is compatible with the categories you're using as gold standard!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with majortopic code 0\n",
    "uk_media = uk_media[uk_media['majortopic'] != 0]\n",
    "\n",
    "# only keep rows below 24 OR equal to 99\n",
    "uk_media = uk_media[(uk_media['majortopic'] < 24) | (uk_media['majortopic'] == 99)]\n",
    "\n",
    "# we'll recode 99 to 999 to match the classifier label\n",
    "uk_media['majortopic'] = uk_media['majortopic'].replace(99, 999)\n",
    "\n",
    "sorted([int(value) for value in uk_media['majortopic'].unique()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the classifier on the first 10 rows\n",
    "classifier(list(uk_media['text'][0:9])) # we need to pass the text column as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the whole dataset\n",
    "classifications = classifier(list(uk_media[\"text\"])) # we need to pass the text column as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as this may take a while we'll safe the classifications locally after the run\n",
    "# the pickle module can safe and load arbitrary Python objects\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/classifications.pkl', 'wb') as f:\n",
    "    pickle.dump(classifications, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then reload the \"pickle\" file when we need the classifcation results again later\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/classifications.pkl', 'rb') as f:\n",
    "    classifications = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the classifications to our UK media dataframe\n",
    "\n",
    "classifications_df = pd.DataFrame(classifications) # turn the classifications a DataFrame\n",
    "\n",
    "uk_media.reset_index(drop=True, inplace=True) # we need to reset the indices to avoid mismatches\n",
    "classifications_df.reset_index(drop=True, inplace=True) #  (the classifications index is a fresh one while the uk_media carries missing indices where we removed the data before)\n",
    "\n",
    "uk_media_classified = pd.concat([uk_media, # combine the two DataFrames\n",
    "                                 classifications_df], \n",
    "                                 axis=1) \n",
    "\n",
    "# for evaluation purposes, we'll also turn the majortopic into strings\n",
    "uk_media_classified['majortopic'] = uk_media_classified['majortopic'].astype(str)\n",
    "\n",
    "uk_media_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Standard\n",
    "Let's take a look how well our model did. Luckily, the UK Media Dataset comes with labels annotated by trained human coders. This is often referred to as the *Gold Standard*, meaning it will be the standard our model will be held to. In other words, our model would be \"perfect\" if it could perfectly recreate the human coding. It is important to note that even another trained human coder is unlikely to recreate the exact same coding. This is due to differences and errors in human judgement. This is why in qualitative research, we use the measure of intercoder reliability to make sure our human coders have substandial agreement on their coding tasks. There are some debates on the matter (see, for example, [this paper](https://doi.org/10.1177/1609406919899220)) and various way to calculate it (with Krippendorf's Alpha being the most common), but usually you want your intercoder reliability to be above 85%, with values above 65% sometimes being considered acceptable, depending on the task and data. For our model, this means that we can usually accept it if it does not reproduce our gold standard to 100% (sometimes, this can even be a sign of overfitting to the data). \n",
    "\n",
    "However, while some researchers argue otherwise, I strongly recommend against using intercoder reliability to judge model output. The main reason for this is that it somewhat relaxes assumptions by calculating the measurement in such a way that the model is considered equally capable of coding the data as a trained human coder. This is simply not the case, as we usually want to find out whether or not our model can meaningfully replicate human coders' judgement. Intercoder reliability is an important tool when producing the gold standard, as it makes sure it is actually a *good* standard, and you should always make use of it when qualitatively coding a data set (even when coding it by yourself, it is good practice to double code a part of your data to make sure your coding is reliable!). But for measuring a model's capabilities, it is strongly adviced to rely on the evaluation metrics developed in the machine learning domain to measure exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is a very simple measure: it is the percentage of correct predictions out of all predictions made by the model. As a formula: \n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "While this is an important metric, it is often not a very good metric if your classes are imbalanced. Accuracy puts higher emphasis on the majority class, which is often not the one we're interested in. Think of a spam filter which is great at predicting which emails are not spam, but fails at predicting the (much rarer case) of an email actually being spam. This is especially a problem in multi-class classification, as your less-well represented classes are not only harder to predict for the model due to a lack of training data, but its failure to predict them will also not show in your evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F1\n",
    "To overcome these limitations, precision, recall and F1 scores were developed. They rely on the differentiation between True and False Positives and True and False Negatives.\n",
    "\n",
    "| <a/> | **Predicted Positive** | **Predicted Negative** |\n",
    "|:--- |:---:|:---:|\n",
    "| **Observed Positive** | True Positives (TP) | False Negatives (FN) |\n",
    "| **Observed Negative** | False Positives (FP) | True Negatives (TN) |\n",
    "\n",
    "- **True Positives (TP)**: The model correctly predicted the positive class.\n",
    "- **True Negatives (TN)**: The model correctly predicted the negative class.\n",
    "- **False Positives (FP)**: The model incorrectly predicted the positive class (predicted positive, but the true label was negative).\n",
    "- **False Negatives (FN)**: The model incorrectly predicted the negative class (predicted negative, but the true label was positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision measures the proportion of True Positives out of all positive predictions made by the model:\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "That is, it measures how many positive predictions were actually correct. Precision is the preferred metric if the cost of false positives is high. For example, if your spam filter automatically deletes any email it identifies as spam, you will want very high precision so it does not accidentally delete your other emails (false positives).\n",
    "\n",
    "However, the problem with precision is that conservative estimates fare better, as they produce less false positives. If your spam filter is not flagging a lot of mails as spam to begin with, it will have high precision, but be also not very good at its job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall measures the proportion of True Positives correctly identified by the model:\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "Recall is the preferred metric when the cost of false negatives is high, for example in medical tests or in flagging hate speech detection where you want to err on the side of caution. \n",
    "\n",
    "The problem with recall, is opposed to that of precision: you can achieve perfect recall by predicting all labels as positive, thus making sure you do not miss any relevant cases.\n",
    "\n",
    "Recall is sometimes also called sensitivity - especially in the medical field. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score is the harmoic mean of precision and recall. That is, it gives an even measure for precision and recall, balancing out their respective weaknesses.\n",
    "\n",
    "$$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "It is usually a good overview score of how well your model fares. If you do not have specific requirements for your model that make you value either precision or recall more highly (that is, if neither the cost of false negatives nor of false positives is higher), you will usually want to go with the F1 score to judge model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "In order to give you a feeling what these values mean: If we want to classify into one of two categories, using a coin flip for a classifier would produce an accuracy, precision, recall and F1 score of 0.5 - that is, there is a 50% chance of predicting the right class. So if your evaluation metrics are below 0.5, you would be better off just flipping a coin. If you have four classes, random chance of predicting the right one would be 25%, and the scores would equally come down to 0.25. \n",
    "\n",
    "So you should keep in mind that what is a \"good enough\" also depends on the number of categories you're trying to predict. However, eventually it depends on your use-case what the critical value for a reliable model is. For social science research, a rule of thumb is that you want your model to be roughly as reliable as human coders, which means values above 0.85 are considered good, but lower values may be acceptable in some cases. But again, this depends on your specific use case and you should think about what it means when, say, your measurements are only correct in two out of three cases (accuracy of 0.65) - and maybe even worse for some less well-represented categories. Can you make meaningful assumptions at this point? Or should you consider using a different method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "While we could calculate all these scores by hand, the scikit-learn library luckily has handy functions to do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"])\n",
    "precision = precision_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"], average=\"weighted\")\n",
    "recall = recall_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"], average=\"weighted\")\n",
    "f1 = f1_score(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"], average=\"weighted\")\n",
    "\n",
    "print(\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'Recall: {recall}\\n'\n",
    "    f'F1: {f1}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We even have a handy overview function for all of this together, and print metrics over all classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(uk_media_classified[\"majortopic\"], uk_media_classified[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics look quite decent in general. However, there are some categories were the model performs worse, mostly those with less cases (support). For example, the model is struggling with category 2 (civil rights), but also 7 (environment), 20 (government operations), 21 (public lands). 23 (culture) is abyssmal, but there are only two cases. 9 (immigration) performs well, even though we do not have a lot of cases in our data. Interestingly, there is no overlap between the gold standard and the mode output for the residue category 999.\n",
    "**Important**: Low support (few cases) here means that we do not have a lot of cases in our data, and therefore our metrics may not be generalizable (the metrics might look different on more data). It does *not* mean that the training data had few of these categories (even though that may well be the case, seeing how they are rare in our data).\n",
    "\n",
    "*Note*: There is also a category 22 which is returned by the model in 2 cases. This is the raw value \"22\" returned by the model, as we did not assign this value. I could not find out what this category entails, as it is not mentioned on the model page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're tracking data over a rather long amount of time, it would be interesting to see if the model fares equally well over the years. We can plot this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each year, and plot the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for year in uk_media_classified['year'].unique():\n",
    "    year_data = uk_media_classified[uk_media_classified['year'] == year]\n",
    "    f1 = f1_score(year_data[\"majortopic\"], year_data[\"label\"], average=\"weighted\")\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "plt.plot(uk_media_classified['year'].unique(), f1_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the model performance dips in some places - especially in the early 60s, but also the early 80s and around the year 2000, and is shaky in the early 2000s. Let's see if this has something to do with the number of articles per year (that is, less reliable metrics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay the plot with the number of articles per year on a differnt scale (right hand-side)\n",
    "article_count = uk_media_classified.groupby('year').size()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('F1 Score', color=color)\n",
    "ax1.plot(uk_media_classified['year'].unique(), f1_scores, color=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Article Count', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(article_count.index, article_count, color=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we see that the articles are not evenly distributed and we have a large dip especially in the late 70s, this does not seem to correlate with the model performance. Therefore, our model is just bad at predicting the topics in certain decades, for example the 80s!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: I was unable to find out how exactly the model was trained, as the authors do not clearly state the data they used in training. However, it is likely that they used this exact data set as part of their training. Therefore, the model may overperform in our test case here, as it already \"knows\" the data (it was trained on it). Model performance may be worse for unkown data, so you should test it extensively when using your own data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_workshop_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
